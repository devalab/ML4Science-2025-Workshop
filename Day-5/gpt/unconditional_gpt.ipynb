{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de40faf",
   "metadata": {},
   "source": [
    "# Uncondtional Generative Pretrained Transformer (GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03461c3",
   "metadata": {},
   "source": [
    "## Setup the notebook and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f7a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, QED, rdMolDescriptors, AllChem, DataStructs\n",
    "from rdkit import RDLogger\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.ERROR)\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea535b5",
   "metadata": {},
   "source": [
    "## Load Dataset, Tokenize SMILES and make PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5897d",
   "metadata": {},
   "source": [
    "Function to load the ESOL dataset and make it a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a6c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_esol_dataset():\n",
    "    url = \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    print(f\"Loaded dataset with {len(df)} compounds\")\n",
    "\n",
    "    df = df[['smiles', 'logP', 'qed', 'SAS']]\n",
    "    df.columns = ['SMILES', 'LogP', 'QED', 'SAS']\n",
    "\n",
    "    # Remove invalid SMILES\n",
    "    valid_smiles = []\n",
    "    for smiles in df['SMILES']:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        valid_smiles.append(mol is not None)\n",
    "\n",
    "    df = df[valid_smiles].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f3875",
   "metadata": {},
   "source": [
    "Tokenizer to convert characters in SMILES strings to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e007ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, smiles_list):\n",
    "        chars = sorted(set(''.join(smiles_list)))\n",
    "        self.itos = ['<pad>', '<bos>', '<eos>'] + chars\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.itos)}\n",
    "\n",
    "    def encode(self, smiles):\n",
    "        return [self.stoi[c] for c in smiles]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return ''.join(self.itos[i] for i in token_ids if i > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88e690",
   "metadata": {},
   "source": [
    "Class to define te input data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b53e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles_list, tokenizer, block_size):\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        for smi in smiles_list:\n",
    "            tokens = tokenizer.encode(smi)\n",
    "            tokens = [tokenizer.stoi[\"<bos>\"]] + tokens + [tokenizer.stoi[\"<eos>\"]]\n",
    "            if len(tokens) > block_size:\n",
    "                continue\n",
    "            tokens += [tokenizer.stoi[\"<pad>\"]] * (block_size - len(tokens))\n",
    "            self.data.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx][:-1])\n",
    "        y = torch.tensor(self.data[idx][1:])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93602e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 249455 compounds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = load_esol_dataset()\n",
    "smiles_list = df['SMILES'].tolist()\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "tokenizer = CharTokenizer(smiles_list)\n",
    "dataset = SmilesDataset(smiles_list, tokenizer, block_size=block_size)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585adcc5",
   "metadata": {},
   "source": [
    "## Defining the model configuration and Instantiating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fc723",
   "metadata": {},
   "source": [
    "Defining the configuration class and the model components\n",
    "\n",
    "*Causal Self Attention* which is a form of Masked Self Attention is used where only the tokens from the past until the present are used to predict the next token. Each block is a unit of the Conditional GPT model which is repeated several times in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ba8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embd_pdrop = 0.1\n",
    "        self.resid_pdrop = 0.1\n",
    "        self.attn_pdrop = 0.1\n",
    "        self.n_layer = 8\n",
    "        self.n_head = 8\n",
    "        self.n_embd = 256\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                      .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        mask = self.mask[:, :, :T, :T].to(att.device)\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89015063",
   "metadata": {},
   "source": [
    "GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32fbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, \"Sequence too long for block size\"\n",
    "\n",
    "        tok_emb = self.tok_emb(idx)\n",
    "        pos_emb = self.pos_emb[:, :t, :]\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
    "        return logits, loss\n",
    "\n",
    "    def sample(self, tokenizer, max_len=120, device='cpu', temperature=1.0, top_k=None, num_samples=1):\n",
    "        self.eval()\n",
    "        out_smiles = []\n",
    "        bos = tokenizer.stoi[\"<bos>\"]\n",
    "        eos = tokenizer.stoi[\"<eos>\"]\n",
    "        pad = tokenizer.stoi[\"<pad>\"]\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            idx = torch.tensor([[bos]], dtype=torch.long, device=device)\n",
    "            generated = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(max_len):\n",
    "                    logits, _ = self.forward(idx)\n",
    "                    logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "                    if top_k is not None:\n",
    "                        # Ensure top_k does not exceed the vocabulary size\n",
    "                        k_val = min(top_k, logits.size(-1))\n",
    "                        values, _ = torch.topk(logits, k_val)\n",
    "                        min_values = values[:, -1].unsqueeze(-1)\n",
    "                        logits = torch.where(logits < min_values, torch.tensor(-1e10, device=logits.device), logits)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    token_id = next_token.item()\n",
    "                    if token_id == eos:\n",
    "                        break\n",
    "                    if token_id == pad:\n",
    "                        continue\n",
    "                    generated.append(token_id)\n",
    "                    idx = torch.cat([idx, next_token], dim=1)\n",
    "            out_smiles.append(tokenizer.decode(generated))\n",
    "        return out_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a43aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "        vocab_size=len(tokenizer.itos),\n",
    "        block_size=block_size,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        n_embd=128\n",
    "    )\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40890768",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1d88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(model, tokenizer, train_loader, num_epochs=10, device='cuda', lr=3e-4):\n",
    "    \"\"\"Train the already initialized model\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(x, targets=y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Sample at epoch end\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sampled = model.sample(tokenizer=tokenizer, max_len=120, device=device, temperature=1.0, num_samples=3)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(f\"  Samples: {sampled[:2]}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9080bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8854\n",
      "  Samples: ['CC[C@@H](/C=C2\\\\Sc3ncccc3C=Cc3ccccc3=[NH+]2)C1C[C@@H]1C\\n', 'CNC(=O)C(=O)Nc1ccc(-c2cccc(CC(C)C)c2)cc1\\n']\n",
      "Epoch 2/10, Loss: 0.6901\n",
      "  Samples: ['CC1(C)[C@H]2CCCN2C(=O)N1CCO[C@H]1C(F)(F)F\\n', 'N3CC[NH+](CC(=O)Nc3ccccc3C2)CC2CC1)[C@@H]2CCC[NH2+]1\\n']\n",
      "Epoch 3/10, Loss: 0.6462\n",
      "  Samples: ['O=C(C[NH+](Cc1ccco1)[C@H]1CCOC2(CC2)C1)NC1CCCCCCCC1\\n', 'C[C@@H](C(N)=O)[NH+](C)Cc1ccc(Cl)cc1\\n']\n",
      "Epoch 4/10, Loss: 0.6220\n",
      "  Samples: ['CCC(CC)NC(=O)c1ccc(NC(=O)Nc2cccc(F)c2F)cc1\\n', 'CCc1nccc(C(=O)N(Cc2cc(C)no2)[C@@H]2CCS(=O)(=O)C2)c1C\\n']\n",
      "Epoch 5/10, Loss: 0.6062\n",
      "  Samples: ['COC(=O)c1cc(C)nc2ccccn12\\n', 'C[C@@H]([NH2+]CC[NH+](C)C)c1ccn(Cc2ccncc2)c1\\n']\n",
      "Epoch 6/10, Loss: 0.5951\n",
      "  Samples: ['COc1cccc(/C=C/C(=O)Nc2ccc(S(N)(=O)=O)c([N+](=O)[O-])c2)c1\\n', 'CCC[NH2+]CC(C)(C)S(=O)(=O)CC\\n']\n",
      "Epoch 7/10, Loss: 0.5864\n",
      "  Samples: ['CC1=Cc2cc([C@]34C[C@H]5CC[C@]4(CC)CC[C@H]4Br)ccc2C(C)C1\\n', 'COc1ccc(OC(=O)c2ccc(S(=O)(=O)N3[C@H]4CCOC[C@@H]4C3)s2)cc1C\\n']\n",
      "Epoch 8/10, Loss: 0.5797\n",
      "  Samples: ['Cc1nc(CN2C[C@@H](C)CO[C@H](CC(F)(F)F)C2)sc1C\\n', 'COC(=O)c1ccccc1NC(=O)Cn1cccn1\\n']\n",
      "Epoch 9/10, Loss: 0.5741\n",
      "  Samples: ['CN(C)c1cccc(CNC(=O)NCc2ncc(-c3cccnc3)o2)n1\\n', 'CC/C=C(=O)C1=C(C)N=C2S[C@H]1c1ccccc1\\n']\n",
      "Epoch 10/10, Loss: 0.5695\n",
      "  Samples: ['COc1ccccc1C[NH2+]Cc1cc(Cl)ccc1OC(F)(F)F\\n', 'C[C@@H]1CCCC[NH+]1Cc1nn(-c2cccnc2)c(=O)c2ccccc12\\n']\n"
     ]
    }
   ],
   "source": [
    "model = train_gpt(model, tokenizer, loader, num_epochs=10, device=device, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0affc82",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a28ad",
   "metadata": {},
   "source": [
    "To evaluate basic metrics like validity, novelty and uniqueness. Also, measure the internal diversity of generated samples by calculating the Tanimoto Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72031d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_basic_metrics(generated_smiles, train_smiles_set):\n",
    "    valid_smiles = []\n",
    "    for s in generated_smiles:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(s)\n",
    "            if mol is not None:\n",
    "                valid_smiles.append(s)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    validity = len(valid_smiles) / len(generated_smiles) if len(generated_smiles) > 0 else 0.0\n",
    "    unique_valid = list(set(valid_smiles))\n",
    "    uniqueness = len(unique_valid) / len(valid_smiles) if valid_smiles else 0.0\n",
    "    novelty = len([s for s in unique_valid if s not in train_smiles_set]) / len(unique_valid) if unique_valid else 0.0\n",
    "\n",
    "    return {\n",
    "        \"validity\": validity,\n",
    "        \"uniqueness\": uniqueness,\n",
    "        \"novelty\": novelty,\n",
    "        \"valid_smiles\": unique_valid\n",
    "    }\n",
    "\n",
    "def internal_diversity(smiles_list):\n",
    "    mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
    "    mols = [m for m in mols if m is not None]\n",
    "    fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=2048) for m in mols]\n",
    "    N = len(fps)\n",
    "    if N < 2:\n",
    "        return 0.0\n",
    "    distances = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            sim = DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
    "            distances.append(1 - sim)\n",
    "    return float(np.mean(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683135d1",
   "metadata": {},
   "source": [
    "Compute KL-divergence of the model to ensure that the generated samples belong to the same (similar) distribution as the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2fd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_properties(smiles_list):\n",
    "    props = []\n",
    "    for s in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol:\n",
    "            props.append([\n",
    "                Descriptors.MolWt(mol),\n",
    "                Descriptors.MolLogP(mol),\n",
    "                QED.qed(mol),\n",
    "                rdMolDescriptors.CalcNumRings(mol)\n",
    "            ])\n",
    "    if len(props) == 0:\n",
    "        return np.zeros((0, 4))\n",
    "    return np.array(props)\n",
    "\n",
    "def kl_divergence(real_props, gen_props, bins=50):\n",
    "    if real_props.shape[0] < 2 or gen_props.shape[0] < 2:\n",
    "        return [float('nan')] * real_props.shape[1]\n",
    "    kl_values = []\n",
    "    for i in range(real_props.shape[1]):\n",
    "        real_hist, _ = np.histogram(real_props[:, i], bins=bins, density=True)\n",
    "        gen_hist, _ = np.histogram(gen_props[:, i], bins=bins, density=True)\n",
    "        real_hist += 1e-9\n",
    "        gen_hist += 1e-9\n",
    "        kl = entropy(real_hist, gen_hist)\n",
    "        kl_values.append(kl)\n",
    "    return kl_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eba46a",
   "metadata": {},
   "source": [
    "Function to evaluate the model based on the above metrics and the functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae243d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, train_smiles, device='cpu',\n",
    "                   num_samples=5000, sample_max_len=120, temperature=1.0, top_k=None):\n",
    "    model.to(device)\n",
    "    print(\"Generating samples...\")\n",
    "    generated = []\n",
    "    batch = 100\n",
    "    reps = max(1, num_samples // batch)\n",
    "    for _ in range(reps):\n",
    "        gen = model.sample(tokenizer=tokenizer, max_len=sample_max_len, device=device,\n",
    "                            temperature=temperature, top_k=top_k, num_samples=batch)\n",
    "        generated.extend(gen)\n",
    "    if len(generated) < num_samples:\n",
    "        extra = num_samples - len(generated)\n",
    "        gen = model.sample(tokenizer=tokenizer, max_len=sample_max_len, device=device,\n",
    "                            temperature=temperature, top_k=top_k, num_samples=extra)\n",
    "        generated.extend(gen)\n",
    "\n",
    "    for i, smi in enumerate(generated):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            generated.remove(smi)\n",
    "\n",
    "    print(\"Computing metrics...\")\n",
    "    train_set = set(train_smiles)\n",
    "    basic = evaluate_basic_metrics(generated, train_set)\n",
    "    valid_smiles = basic['valid_smiles']\n",
    "    diversity_score = internal_diversity(valid_smiles)\n",
    "\n",
    "    real_props = compute_properties(train_smiles[:5000])\n",
    "    gen_props = compute_properties(valid_smiles[:5000])\n",
    "\n",
    "    if real_props.size == 0 or gen_props.size == 0:\n",
    "        kl = [float('nan')] * 4\n",
    "    else:\n",
    "        kl = kl_divergence(real_props, gen_props)\n",
    "\n",
    "    return {\n",
    "        \"validity\": basic[\"validity\"],\n",
    "        \"uniqueness\": basic[\"uniqueness\"],\n",
    "        \"novelty\": basic[\"novelty\"],\n",
    "        \"diversity\": diversity_score,\n",
    "        \"kl_divergence\": {\n",
    "            \"MolWt\": kl[0] if len(kl) > 0 else float('nan'),\n",
    "            \"LogP\": kl[1] if len(kl) > 1 else float('nan'),\n",
    "            \"QED\": kl[2] if len(kl) > 2 else float('nan'),\n",
    "            \"NumRings\": kl[3] if len(kl) > 3 else float('nan')\n",
    "        },\n",
    "        \"raw_generated_examples\": generated[:20]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a010",
   "metadata": {},
   "source": [
    "## Generate and Evaluate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7caae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples...\n",
      "Computing metrics...\n",
      "\n",
      "Validity: 0.996\n",
      "Uniqueness: 1.000\n",
      "Novelty: 0.998\n",
      "Diversity: 0.876\n"
     ]
    }
   ],
   "source": [
    "# evaluate the generated samples\n",
    "metrics = evaluate_model(model, tokenizer, smiles_list, device=device,\n",
    "                        num_samples=1000, temperature=1.0, top_k=50)\n",
    "\n",
    "print(f\"\\nValidity: {metrics['validity']:.3f}\")\n",
    "print(f\"Uniqueness: {metrics['uniqueness']:.3f}\")\n",
    "print(f\"Novelty: {metrics['novelty']:.3f}\")\n",
    "print(f\"Diversity: {metrics['diversity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24ac8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binding-pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
